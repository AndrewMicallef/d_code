"""These routines are for doing typical extracellular (cell-attached) analyses.

The main data structures we use here are XSG dictionaries.  These contain keys
by default that have a collection of meta-data, and one key for each of the three
ephus programs (ephys, acquierer and stimulator).

We have been inspired by the spike_sort package, but re-implemented routines to better
fit the XSG data structure.  In particular, we have 'detectSpikes' and 'extractSpikes', 
as well as routines to calculate spike rate histograms and densities, and plotting a 
spike raster.





"""
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats

from itertools import repeat

import copy

__all__ = ['plotRaster', 'make_STH', 'make_spike_density', 'detectSpikes', 'extract_spikes']

def detectSpikes(orig_xsg, thresh=None, edge='falling', channel='chan0', filter_trace=False):
    # extract spike times and add a field called 'spikeTimes'

    # needs to take a merged or un-merged XSG and add a field called 'spike_times'
    # if unmerged, spike_times is a single np.ndarray of spike times (in samples), otherwise
    # it is a list of such np.ndarrays.
    
    # returns a new xsg with the added key.

    assert(edge in ['falling', 'rising'], "Edge must be 'falling' or 'rising'!")
    xsg = copy.deepcopy(orig_xsg)

    # internal function to be used with a map
    def detect(params): 
        trace, thresh, filter_trace = params

        if filter_trace:
            #trace = filterthetrace(trace)
            pass

        # thresh is now a single value or an explicit wave the same size and shape as trace
        # let's just make it explicit
        if type(thresh) is not np.ndarray:
            thresh = np.ones_like(trace) * thresh
        
        if edge == 'rising':
            i, = np.where((trace[:-1] < thresh[:-1]) & (trace[1:] > thresh[1:]))
        if edge == 'falling':
            i, = np.where((trace[:-1] > thresh[:-1]) & (trace[1:] < thresh[1:]))
        return i
                
    if 'merged' in xsg.keys():
        # important type expectation here --- could be list of floats or a list of expicit ndarrays
        if type(thresh) is not list:  
            thresh = repeat(thresh)
        if type(filter_trace) is not list:
            filter_trace = repeat(filter_trace)

        xsg['spikeTimes'] = map(detect, zip(np.rollaxis(xsg['ephys'][channel], 1, 0), thresh, filter_trace))
    else:
        xsg['spikeTimes'] = detect((xsg['ephys'][channel], thresh, filter_trace)) # wrapping here to make it compatible with the zip for a single trial

    return xsg

def extract_spikes():
    pass

def plotRaster(xsg, ax=None, height=1.):
    """Creates raster plot from a merged or unmerged XSG dictionary.
    Note that the dictionary has to have the key 'spikeTimes', which is 
    generated by detectSpikes().  The dictionary is a single numpy array
    with spike loctions in samples (or a list of such arrays).

    Note that we plot these based on the size of the traces themselves.
    This works because we split up our acquisitions, but in general, 
    we might want to only plot regions of the raster.  plt.xlim() should
    be able to be used post-hoc for this.

    :param: - xsg - a merged or unmerged XSG dictionary with a 'spikeTimes' entry
    :param: - ax - optional, a matplotlib axis to plot onto
    :param: - height - optional, spacing for the rasters
    """
    if ax is None:
        ax = plt.gca() # otherwise it'll plot on the top figure

    try:
        if type(xsg['spikeTimes']) is list:
            for trial, trace in enumerate(xsg['spikeTimes']):
                plt.vlines(trace, trial, trial+height)
            plt.ylim(0,len(xsg['spikeTimes']))

        else:
            plt.vlines(0, xsg['spikeTimes'], height)
            plt.ylim((0,1))

        plt.xlim(0,xsg['ephys']['chan0'].shape[0])
        plt.xlabel('time (samples)')
        plt.ylabel('trials')

    except:
        print 'No spike times found!'

def make_STH(spike_times, bin_size=0.25, win=None, n_trials=None, rate=False, **kwargs):
    """Parameters:
        spike_times - a list of spike time dictionaries, or a single dictionary
        bin_size - float, in ms
        win - tuple of trial length in ms, eg: (0, 30000)
        ntrials - number of trials to plot (if None plot all)

    Returns:
        STH - bins x # of spikes
        bins - bin values, in ms, use for plotting
    """
    if isinstance(spike_times, dict):
        spike_times = [spike_times]
    if n_trials is None:
        n_trials=len(spike_times)
    
    bins = np.arange(win[0],win[1],bin_size)
    
    STH = np.empty((bins.shape[0], n_trials))
    for trial in range(n_trials):
        trial_sth, bins = np.histogram(spike_times[trial]['data'], bins)
        
        if rate:
            trial_sth = trial_sth * 1. / bin_size * 1000.
        
        STH[:-1,trial] = trial_sth
    return STH, bins

def make_spike_density(sth, sigma=1):
    edges = np.arange(-3*sigma, 3*sigma, 0.001)
    kernel = stats.norm.pdf(edges, 0, sigma)
    kernel *= 0.001
    
    center = np.ceil(edges.shape[0]/2)
    center = int(center)
    
    spike_density = np.empty_like(sth)
    for i, trial in enumerate(np.rollaxis(sth, 1)):
        
        conv_spikes = np.convolve(trial.copy(), kernel)
        spike_density[:,i] = conv_spikes[center:-center+1]
        
    return spike_density
